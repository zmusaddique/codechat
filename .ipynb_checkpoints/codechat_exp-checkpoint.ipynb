{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a66bed35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import download_loader\n",
    "from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",  # replace with your model name\n",
    "    context_window=2048,  # to use refine\n",
    "    token=os.getenv('HUGGINGFACEHUB_API_TOKEN'),  # replace with your HuggingFace token\n",
    ")\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=\"local:BAAI/bge-small-en-v1.5\", chunk_size=1024, chunk_overlap=64)\n",
    "\n",
    "# from llama_index.embeddings import HuggingFaceInferenceAPIEmbedding\n",
    "\n",
    "# embed_model1 = HuggingFaceInferenceAPIEmbedding(\n",
    "#                 model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model1)\n",
    "\n",
    "node_parser = service_context.node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58f83b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "#     model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512}\n",
    "# )\n",
    "\n",
    "# query = \"Who is elon musk? from a scientist perspective\"\n",
    "\n",
    "# prompt = f\"\"\"\n",
    "#  <|system|>\n",
    "# You are an AI assistant that follows instruction extremely well.\n",
    "# Please be truthful and give direct answers\n",
    "# </s>\n",
    "#  <|user|>\n",
    "#  {query}\n",
    "#  </s>\n",
    "#  <|assistant|>\n",
    "# \"\"\"\n",
    "\n",
    "# response = llm.predict(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efa90c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch and set API keys\n",
    "# active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "# dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "# github_token = os.getenv('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d84256fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_loop_token, dataset_path, github_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43f7600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_github_url(url):\n",
    "    pattern = r\"https:\\/\\/github\\.com\\/([^/]+)\\/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e18bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_loop_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb1b9188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zmusaddique', 'chatbot-restaurant')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_github_url(\"https://github.com/zmusaddique/chatbot-restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40763dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b42f58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_github_client():\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e13997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2ecbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6587db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index import PromptTemplate\n",
    "# get retrievers\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "from tqdm.asyncio import tqdm\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "import asyncio\n",
    "from llama_index.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "import chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5fac974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a016be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(llm, query_str, num_queries=4)\n",
    "        results = run_queries(queries, [vector_retriever, bm25_retriever])\n",
    "        final_results = fuse_results(\n",
    "            results_dict, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01b00598",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "        \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "        \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "        \"related to the following input query:\\n\"\n",
    "        \"Query: {query}\\n\"\n",
    "        \"Queries:\\n\"\n",
    "    )\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b876a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from llama_index import download_loader\n",
    "# download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "# from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "\n",
    "# github_client = GithubClient('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')\n",
    "# loader = GithubRepositoryReader(\n",
    "#     github_client,\n",
    "#     owner =                  \"jerryjliu\",\n",
    "#     repo =                   \"llama_index\",\n",
    "#     filter_directories =     ([\"llama_index\", \"docs\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "#     filter_file_extensions = ([\".py\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "#     verbose =                True,\n",
    "#     concurrent_requests =    10,\n",
    "# )\n",
    "\n",
    "# # docs = loader.load_data(branch=\"main\")\n",
    "# # alternatively, load from a specific commit:\n",
    "# docs = loader.load_data(commit_sha=\"c4cbb7d361bbacbbddd93742e57fd9aaba20b65e\")\n",
    "\n",
    "# for doc in docs:\n",
    "#     print(doc.extra_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f12eb8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader loaded\n",
      "Loading chatbot-restaurant repository by zmusaddique\n",
      "Documeznts uploaded: \n",
      "{'file_path': 'FoodChatBot/db_helper.py', 'file_name': 'db_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/db_helper.py'}\n",
      "{'file_path': 'FoodChatBot/frontend/frontend_server.py', 'file_name': 'frontend_server.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/frontend/frontend_server.py'}\n",
      "{'file_path': 'FoodChatBot/generic_helper.py', 'file_name': 'generic_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/generic_helper.py'}\n",
      "{'file_path': 'FoodChatBot/main.py', 'file_name': 'main.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/main.py'}\n",
      "{'file_path': 'README.md', 'file_name': 'README.md', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/README.md'}\n",
      "{'file_path': 'backend/db_helper.py', 'file_name': 'db_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/db_helper.py'}\n",
      "{'file_path': 'backend/extra/extra.py', 'file_name': 'extra.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/extra/extra.py'}\n",
      "{'file_path': 'backend/generic_helper.py', 'file_name': 'generic_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/generic_helper.py'}\n",
      "{'file_path': 'backend/main.py', 'file_name': 'main.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/main.py'}\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'service_context' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtextwrap\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;28mstr\u001b[39m(streaming_response),\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 168\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m#     main()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nest_asyncio.py:31\u001b[0m, in \u001b[0;36m_patch_asyncio.<locals>.run\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     29\u001b[0m task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mensure_future(main)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task\u001b[38;5;241m.\u001b[39mdone():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/nest_asyncio.py:99\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f\u001b[38;5;241m.\u001b[39mdone():\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent loop stopped before Future completed.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/futures.py:201\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__log_traceback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception_tb)\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mcoro\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[38;5;241m=\u001b[39m coro\u001b[38;5;241m.\u001b[39mthrow(exc)\n",
      "Cell \u001b[0;32mIn[34], line 38\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#             nodes = node_parser.get_nodes_from_documents(docs)\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m             nodes \u001b[38;5;241m=\u001b[39m \u001b[43mservice_context\u001b[49m\u001b[38;5;241m.\u001b[39mnode_parser\u001b[38;5;241m.\u001b[39mget_nodes_from_documents(docs)\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m \u001b[38;5;66;03m# Exit the loop once the valid URL is processed\u001b[39;00m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'service_context' referenced before assignment"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    huggingfacehub_api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    if not huggingfacehub_api_token:\n",
    "        raise EnvironmentError(\"HuggingFaceHub API key not found in enivronment variables\")\n",
    "        \n",
    "    github_token = os.getenv('GITHUB_TOKEN')\n",
    "    if not github_token:\n",
    "        raise EnvironmentError(\"Github token not found in environment variables\")\n",
    "        \n",
    "    active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "    if not active_loop_token:\n",
    "        raise EnvironmentError(\"Activloop token not found in environment variables\")\n",
    "        \n",
    "    github_client = initialize_github_client()\n",
    "    download_loader(\"GithubRepositoryReader\")\n",
    "    print(\"loader loaded\")\n",
    "    github_url = \"https://github.com/zmusaddique/chatbot-restaurant\"\n",
    "    \n",
    "    while True:\n",
    "        owner, repo = parse_github_url(github_url)\n",
    "        if validate_owner_repo(owner, repo):\n",
    "            loader = GithubRepositoryReader(\n",
    "                github_client,\n",
    "                owner=owner,\n",
    "                repo=repo,\n",
    "                filter_file_extensions=([\".py\", \".js\", \".ts\", \".md\", \".ipynb\"],\n",
    "                                       GithubRepositoryReader.FilterType.INCLUDE,),\n",
    "                verbose=False,\n",
    "                concurrent_requests=20,\n",
    "            )\n",
    "            print(f\"Loading {repo} repository by {owner}\")\n",
    "            docs = loader.load_data(branch=\"main\")\n",
    "            \n",
    "            print(\"Documeznts uploaded: \")\n",
    "            for doc in docs:\n",
    "                print(doc.metadata)\n",
    "#             nodes = node_parser.get_nodes_from_documents(docs)\n",
    "            nodes = service_context.node_parser.get_nodes_from_documents(docs)\n",
    "            break # Exit the loop once the valid URL is processed\n",
    "        else:\n",
    "            print(\"Invalid Github URL. Please try again.\")\n",
    "            github_url = \"https://github.com/zmusaddique/chatbot-restaurant\"\n",
    "            #github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "            \n",
    "\n",
    "            \n",
    "    print(\"Uploading to vector store...\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------Create vector store and upload data---------------\n",
    "    \n",
    "    chroma_client = chromadb.EphemeralClient()\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"codechat\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#     vector_index = VectorStoreIndex.from_documents(\n",
    "#         docs, \n",
    "#         storage_context=storage_context, \n",
    "#         service_context=service_context, \n",
    "#         show_progress=True,\n",
    "#     )\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "#     index = VectorStoreIndex.from_documents(docs, storage_context=storage_context,llm=llm, )\n",
    "#     index = VectorStoreIndex.from_documents(documents=docs, service_context=service_context)\n",
    "#     vector_index = VectorStoreIndex.from_documents(nodes, storage_context=storage_context, service_context=service_context, show_progress=True,)\n",
    "    vector_index = VectorStoreIndex(\n",
    "        docs, \n",
    "        storage_context=storage_context, \n",
    "        service_context=service_context, \n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(\"vector_index created\")\n",
    "#     query_engine = vector_index.as_query_engine(streaming=True, similarity_top_k=10)\n",
    "#     query_engine = vector_index.as_query_engine(similarity_top_k=10)\n",
    "    \n",
    "#     query_engine_tools = [\n",
    "#         QueryEngineTool(\n",
    "#             query_engine=query_engine,\n",
    "#             metadata=ToolMetadata(\n",
    "#                 name=\"GitHub Repo\",\n",
    "#                 description=\"The Github repo of a project\",\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "    \n",
    "#     query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "#         query_engine_tools=query_engine_tools,\n",
    "#         service_context=service_context,\n",
    "#         use_async=True,\n",
    "#     )\n",
    "    #++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # Fusion\n",
    "    print(\"Fusion starting\")\n",
    "    query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\"\n",
    "#     query_gen_prompt_str = (\n",
    "#         \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "#         \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "#         \"related to the following input query:\\n\"\n",
    "#         \"Query: {query}\\n\"\n",
    "#         \"Queries:\\n\"\n",
    "#     )\n",
    "#     query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "    \n",
    "\n",
    "    ## vector retriever\n",
    "    vector_retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "    ## bm25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=nodes,\n",
    "        similarity_top_k=2\n",
    "    )\n",
    "    \n",
    "    # will retrieve context from specific companies\n",
    "    #nodes = bm25_retriever.retrieve(\"What happened at Viaweb and Interleaf?\")\n",
    "#     for node in nodes:\n",
    "#         display_source_node(node)\n",
    "    query_str = \"What is the repository about and what is the tech stack?\"\n",
    "    queries = generate_queries(llm, query_str, num_queries=4)\n",
    "    results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])\n",
    "    print(results_dict)\n",
    "    final_results = fuse_results(results_dict)\n",
    "    \n",
    "    for n in final_results:\n",
    "        display_source_node(n, source_length=500)\n",
    "        \n",
    "    fusion_retriever = FusionRetriever(\n",
    "        llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    "    )\n",
    "    \n",
    "    response_synthesizer = get_response_synthesizer(service_context=service_context)    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        fusion_retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_str)\n",
    "    print(str(response))\n",
    "    \n",
    "    #++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "#     Include a simple question to test\n",
    "    intro_question = \"What is the repository about?\"\n",
    "    print(f\"Test question: {intro_question}\")\n",
    "    print('=' * 50)\n",
    "    streaming_response = query_engine.query(intro_question)\n",
    "    #streaming_response.print_response_stream()\n",
    "    \n",
    "    print(f\"Answer: {textwrap.fill(str(streaming_response),100)} \\n\")\n",
    "    while True:\n",
    "        user_question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Exiting, Thanks for chatting!\")\n",
    "            break\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Your question: {user_question}\")\n",
    "        \n",
    "        streaming_response = query_engine.query(user_question)\n",
    "        #streaming_response.print_response_stream()\n",
    "        print(f\"Answer: {textwrap.fill(str(streaming_response), 100)} \\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a15dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672397a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754591bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de653f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c4090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3091c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b4672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90679a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1accb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fef245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6f10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "099d261d",
   "metadata": {},
   "source": [
    "# Fusion Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ea136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "github_client = initialize_github_client()\n",
    "download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "github_url = \"https://github.com/zmusaddique/chatbot-restaurant\"\n",
    "owner, repo = parse_github_url(github_url)\n",
    "\n",
    "if validate_owner_repo(owner, repo):\n",
    "    loader = GithubRepositoryReader(\n",
    "        github_client,\n",
    "        owner=owner,\n",
    "        repo=repo,\n",
    "        filter_file_extensions=([\".py\", \".js\", \".ts\", \".md\", \".ipynb\"],\n",
    "                               GithubRepositoryReader.FilterType.INCLUDE,),\n",
    "        verbose=False,\n",
    "        concurrent_requests=10,\n",
    "    )\n",
    "    print(f\"Loading {repo} repository by {owner}\")\n",
    "    docs = loader.load_data(branch=\"main\")\n",
    "    print(\"Documeznts uploaded: \")\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=False,\n",
    "        runtime={\"tensor_db\":True},\n",
    "    )\n",
    "    \n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context, service_context=service_context, show_progress=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e6062",
   "metadata": {},
   "source": [
    "## Query Generation/Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3311b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\"\n",
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "queries = generate_queries(llm, query_str, num_queries=4)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8942a",
   "metadata": {},
   "source": [
    "## Perform Vector Search for Each Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb15e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42bf85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get retrievers\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f44798",
   "metadata": {},
   "source": [
    "#### Context\n",
    "reciprocal rank fusion: for each node, add up its reciprocal rank in every list where it’s retrieved.\n",
    "\n",
    "Then reorder nodes by highest score to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb41ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99357067",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ping www.google.com"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fba85550",
   "metadata": {},
   "source": [
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import download_loader\n",
    "from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch and set API keys\n",
    "active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "\n",
    "\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)\n",
    "\n",
    "\n",
    "def initialize_github_client():\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check for GitHub Token\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not github_token:\n",
    "        raise EnvironmentError(\"GitHub token not found in environment variables\")\n",
    "\n",
    "    # Check for Activeloop Token\n",
    "    active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "    if not active_loop_token:\n",
    "        raise EnvironmentError(\"Activeloop token not found in environment variables\")\n",
    "\n",
    "    github_client = initialize_github_client()\n",
    "    download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "    github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "    owner, repo = parse_github_url(github_url)\n",
    "\n",
    "    while True:\n",
    "        owner, repo = parse_github_url(github_url)\n",
    "        if validate_owner_repo(owner, repo):\n",
    "            loader = GithubRepositoryReader(\n",
    "                github_client,\n",
    "                owner=owner,\n",
    "                repo=repo,\n",
    "                filter_file_extensions=(\n",
    "                    [\".py\", \".js\", \".ts\", \".md\"],\n",
    "                    GithubRepositoryReader.FilterType.INCLUDE,\n",
    "                ),\n",
    "                verbose=False,\n",
    "                concurrent_requests=5,\n",
    "            )\n",
    "            print(f\"Loading {repo} repository by {owner}\")\n",
    "            docs = loader.load_data(branch=\"main\")\n",
    "            print(\"Documents uploaded:\")\n",
    "            for doc in docs:\n",
    "                print(doc.metadata)\n",
    "            break  # Exit the loop once the valid URL is processed\n",
    "        else:\n",
    "            print(\"Invalid GitHub URL. Please try again.\")\n",
    "            github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "\n",
    "    print(\"Uploading to vector store...\")\n",
    "\n",
    "    # ====== Create vector store and upload data ======\n",
    "\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=True,\n",
    "        runtime={\"tensor_db\": True},\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # Include a simple question to test.\n",
    "    intro_question = \"What is the repository about?\"\n",
    "    print(f\"Test question: {intro_question}\")\n",
    "    print(\"=\" * 50)\n",
    "    answer = query_engine.query(intro_question)\n",
    "\n",
    "    print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "    while True:\n",
    "        user_question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if user_question.lower() == \"exit\":\n",
    "            print(\"Exiting, thanks for chatting!\")\n",
    "            break\n",
    "\n",
    "        print(f\"Your question: {user_question}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        answer = query_engine.query(user_question)\n",
    "        print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc5d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
