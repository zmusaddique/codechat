{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a66bed35",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c2579433f349e690e33fa9b3ed94a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0fbb7a947364f25b87abc2bfa1fd619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadeeb87d4cc47db977f4187dcbf9624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a048f05122e4bc38449d3179f101f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e5feadd01f4beea31d27b17628979e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "import re\n",
    "\n",
    "from llama_index.llms import HuggingFaceInferenceAPI, HuggingFaceLLM\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index import download_loader\n",
    "from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",  # replace with your model name\n",
    "    context_window=2048,  # to use refine\n",
    "    token=os.getenv('HUGGINGFACEHUB_API_TOKEN'),  # replace with your HuggingFace token\n",
    ")\n",
    "# llm = HuggingFaceLLM(\n",
    "#     context_window=2048,\n",
    "#     max_new_tokens=256,\n",
    "#     generate_kwargs={\"temperature\": 0.25, \"do_sample\": False},\n",
    "#     tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "#     device_map=\"auto\",\n",
    "#     tokenizer_kwargs={\"max_length\": 2048},\n",
    "#     # uncomment this if using CUDA to reduce memory usage\n",
    "#     # model_kwargs={\"torch_dtype\": torch.float16}\n",
    "# )\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model, chunk_size=1024, chunk_overlap=64)\n",
    "\n",
    "# from llama_index.embeddings import HuggingFaceInferenceAPIEmbedding\n",
    "\n",
    "# embed_model1 = HuggingFaceInferenceAPIEmbedding(\n",
    "#                 model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# service_context = ServiceContext.from_defaults(llm=llm, embed_model=embed_model1)\n",
    "\n",
    "node_parser = service_context.node_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224de4f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b833c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = HuggingFaceHub(\n",
    "#     repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\", \n",
    "#     model_kwargs={\"temperature\": 0.5, \"max_length\": 64,\"max_new_tokens\":512}\n",
    "# )\n",
    "\n",
    "# query = \"Who is elon musk? from a scientist perspective\"\n",
    "\n",
    "# prompt = f\"\"\"\n",
    "#  <|system|>\n",
    "# You are an AI assistant that follows instruction extremely well.\n",
    "# Please be truthful and give direct answers\n",
    "# </s>\n",
    "#  <|user|>\n",
    "#  {query}\n",
    "#  </s>\n",
    "#  <|assistant|>\n",
    "# \"\"\"\n",
    "\n",
    "# response = llm.predict(prompt)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa90c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch and set API keys\n",
    "# active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "# dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "# github_token = os.getenv('GITHUB_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84256fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_loop_token, dataset_path, github_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43f7600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_github_url(url):\n",
    "    pattern = r\"https:\\/\\/github\\.com\\/([^/]+)\\/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e18bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# active_loop_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1b9188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('zmusaddique', 'chatbot-restaurant')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_github_url(\"https://github.com/zmusaddique/chatbot-restaurant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40763dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42f58ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_github_client():\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a8cd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e13997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    global query_gen_prompt\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2ecbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6587db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "from llama_index import PromptTemplate\n",
    "# get retrievers\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "from tqdm.asyncio import tqdm\n",
    "from llama_index.response.notebook_utils import display_source_node\n",
    "from llama_index import QueryBundle\n",
    "from llama_index.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "import asyncio\n",
    "from llama_index.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "import chromadb\n",
    "from llama_index.vector_stores import ChromaVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5fac974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a016be2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionRetriever(BaseRetriever):\n",
    "    \"\"\"Ensemble retriever with fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm,\n",
    "        retrievers: List[BaseRetriever],\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._retrievers = retrievers\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        queries = generate_queries(llm, query_str, num_queries=4)\n",
    "        results = run_queries(queries, self._retrievers)\n",
    "        final_results = fuse_results(\n",
    "            results_dict, similarity_top_k=self._similarity_top_k\n",
    "        )\n",
    "\n",
    "        return final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01a64de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b876a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# from llama_index import download_loader\n",
    "# download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "# from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "\n",
    "# github_client = GithubClient('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')\n",
    "# loader = GithubRepositoryReader(\n",
    "#     github_client,\n",
    "#     owner =                  \"jerryjliu\",\n",
    "#     repo =                   \"llama_index\",\n",
    "#     filter_directories =     ([\"llama_index\", \"docs\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "#     filter_file_extensions = ([\".py\"], GithubRepositoryReader.FilterType.INCLUDE),\n",
    "#     verbose =                True,\n",
    "#     concurrent_requests =    10,\n",
    "# )\n",
    "\n",
    "# # docs = loader.load_data(branch=\"main\")\n",
    "# # alternatively, load from a specific commit:\n",
    "# docs = loader.load_data(commit_sha=\"c4cbb7d361bbacbbddd93742e57fd9aaba20b65e\")\n",
    "\n",
    "# for doc in docs:\n",
    "#     print(doc.extra_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077c27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4cbe8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3951c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978b93c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d448675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_token_presence():\n",
    "    huggingfacehub_api_token = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "    if not huggingfacehub_api_token:\n",
    "        raise EnvironmentError(\"HuggingFaceHub API key not found in enivronment variables\")\n",
    "        \n",
    "    github_token = os.getenv('GITHUB_TOKEN')\n",
    "    if not github_token:\n",
    "        raise EnvironmentError(\"Github token not found in environment variables\")\n",
    "        \n",
    "    active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "    if not active_loop_token:\n",
    "        raise EnvironmentError(\"Activloop token not found in environment variables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12eb8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loader loaded\n",
      "Enter the URL of the github repohttps://github.com/zmusaddique/chatbot-restaurant\n",
      "Loading chatbot-restaurant repository by zmusaddique\n",
      "Documeznts uploaded: \n",
      "{'file_path': 'FoodChatBot/db_helper.py', 'file_name': 'db_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/db_helper.py'}\n",
      "{'file_path': 'FoodChatBot/frontend/frontend_server.py', 'file_name': 'frontend_server.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/frontend/frontend_server.py'}\n",
      "{'file_path': 'FoodChatBot/generic_helper.py', 'file_name': 'generic_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/generic_helper.py'}\n",
      "{'file_path': 'FoodChatBot/main.py', 'file_name': 'main.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/main.py'}\n",
      "{'file_path': 'README.md', 'file_name': 'README.md', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/README.md'}\n",
      "{'file_path': 'backend/db_helper.py', 'file_name': 'db_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/db_helper.py'}\n",
      "{'file_path': 'backend/extra/extra.py', 'file_name': 'extra.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/extra/extra.py'}\n",
      "{'file_path': 'backend/generic_helper.py', 'file_name': 'generic_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/generic_helper.py'}\n",
      "{'file_path': 'backend/main.py', 'file_name': 'main.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/main.py'}\n",
      "Uploading to vector store...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf4d94518c134904a3c549c87c4877e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Add of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Add of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Add of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Add of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Add of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Add of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Add of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Add of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n",
      "Add of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Add of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Add of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Add of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Add of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Add of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Add of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Add of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Add of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n",
      "Add of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Add of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Add of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Add of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Add of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Add of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Add of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Add of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Add of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n",
      "Add of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Add of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Add of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Add of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Add of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Add of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Add of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Add of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Add of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n",
      "Add of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Add of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Add of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Add of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Add of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Add of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Add of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Add of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Add of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n",
      "Add of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Add of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Add of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Add of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Add of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Add of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Add of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Add of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Add of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n",
      "Insert of existing embedding ID: 5309463659715fc83cccd85bfdfd4329dad81226\n",
      "Insert of existing embedding ID: f03614338e520da97416e3295ae1fe31a7ab070c\n",
      "Insert of existing embedding ID: feba273ae8242f5af087a590ab0efd06ca306689\n",
      "Insert of existing embedding ID: 3c9f9c583f12946724d3af3b8846eee74446b6f9\n",
      "Insert of existing embedding ID: e4a1baa2f2847b3fed6676ef703ec551bbac1933\n",
      "Insert of existing embedding ID: a47a1233ca6f435d767e547ae4453940d27281ae\n",
      "Insert of existing embedding ID: a10d4792cdcb5abd6e75393ad37fe4e9557dbe53\n",
      "Insert of existing embedding ID: 7a66d7f8025ff83bb3e5750d5c2350df298da85f\n",
      "Insert of existing embedding ID: f6ab3365f28e29b41bc4aa9d1b9cf126403bd6bd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector_index created\n",
      "Fusion starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 6/6 [00:00<00:00, 72.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** e4a1baa2f2847b3fed6676ef703ec551bbac1933<br>**Similarity:** 0.03306010928961749<br>**Text:** # Dialogflow Chatbot with FastAPI Backend\n",
       "\n",
       "This project is a chatbot implemented using Dialogflow for natural language understanding and FastAPI for the backend server. It allows users to interact with the chatbot for placing food orders, tracking orders, adding and removing items from their order, and completing orders. This README provides an overview of the project's structure and usage.\n",
       "\n",
       "## Table of Contents\n",
       "\n",
       "- [Project Overview](#project-overview)\n",
       "- [Setup and Installation](#setup-and-in...<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Node ID:** 7a66d7f8025ff83bb3e5750d5c2350df298da85f<br>**Similarity:** 0.016666666666666666<br>**Text:** # Author: Dhaval Patel. Codebasics YouTube Channel\r\n",
       "\r\n",
       "import re\r\n",
       "\r\n",
       "def get_str_from_food_dict(food_dict: dict):\r\n",
       "    result = \", \".join([f\"{int(value)} {key}\" for key, value in food_dict.items()])\r\n",
       "    return result\r\n",
       "\r\n",
       "\r\n",
       "def extract_session_id(session_str: str):\r\n",
       "    match = re.search(r\"/sessions/(.*?)/contexts/\", session_str)\r\n",
       "    if match:\r\n",
       "        extracted_string = match.group(0)\r\n",
       "        return extracted_string\r\n",
       "\r\n",
       "    return \"\"<br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The repository is about creating a chatbot using Dialogflow for natural language understanding and FastAPI for the backend server. Users can interact with the chatbot for placing food orders, tracking orders, adding and removing items from their order, and completing orders. The tech stack includes Dialogflow for natural language processing and FastAPI for the backend server.\n",
      "Please enter your question (or type 'exit' to quit): Are there POST requests being made in FASTAPI?\n",
      "==================================================\n",
      "Your question: Are there POST requests being made in FASTAPI?\n",
      "Answer:  Yes, the FastAPI server in this project handles incoming POST requests from Dialogflow, processes\n",
      "them, and returns JSON responses based on the user's intent. The main entry point for handling these\n",
      "requests is the `/` endpoint in the `main.py` file. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "async def main():\n",
    "    validate_token_presence()\n",
    "    github_client = initialize_github_client()\n",
    "    download_loader(\"GithubRepositoryReader\")\n",
    "    print(\"loader loaded\")\n",
    "    \n",
    "    github_url = input(\"Enter the URL of the github repo\")\n",
    "#     github_url = \"https://github.com/zmusaddique/chatbot-restaurant\"\n",
    "    \n",
    "    while True:\n",
    "        owner, repo = parse_github_url(github_url)\n",
    "        if validate_owner_repo(owner, repo):\n",
    "            loader = GithubRepositoryReader(\n",
    "                github_client,\n",
    "                owner=owner,\n",
    "                repo=repo,\n",
    "                filter_file_extensions=([\".py\", \".js\", \".ts\", \".md\", \".ipynb\"],\n",
    "                                       GithubRepositoryReader.FilterType.INCLUDE,),\n",
    "                verbose=False,\n",
    "                concurrent_requests=20,\n",
    "            )\n",
    "            print(f\"Loading {repo} repository by {owner}\")\n",
    "            docs = loader.load_data(branch=\"main\")\n",
    "            \n",
    "            print(\"Documeznts uploaded: \")\n",
    "            for doc in docs:\n",
    "                print(doc.metadata)\n",
    "            service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model)\n",
    "            nodes = service_context.node_parser.get_nodes_from_documents(docs)\n",
    "            break # Exit the loop once the valid URL is processed\n",
    "        else:\n",
    "            print(\"Invalid Github URL. Please try again.\")\n",
    "            github_url = \"https://github.com/zmusaddique/chatbot-restaurant\"\n",
    "            #github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "            \n",
    "\n",
    "            \n",
    "    print(\"Uploading to vector store...\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # -------------Create vector store and upload data---------------\n",
    "    \n",
    "    chroma_client = chromadb.EphemeralClient()\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"codechat\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "#     vector_index = VectorStoreIndex.from_documents(\n",
    "#         docs, \n",
    "#         storage_context=storage_context, \n",
    "#         service_context=service_context, \n",
    "#         show_progress=True,\n",
    "#     )\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    storage_context.docstore.add_documents(nodes)\n",
    "#     index = VectorStoreIndex.from_documents(documents=docs, service_context=service_context)\n",
    "#     vector_index = VectorStoreIndex.from_documents(nodes, storage_context=storage_context, service_context=service_context, show_progress=True,)\n",
    "    vector_index = VectorStoreIndex(\n",
    "        docs, \n",
    "        storage_context=storage_context, \n",
    "        service_context=service_context, \n",
    "        show_progress=True,\n",
    "    )\n",
    "    print(\"vector_index created\")\n",
    "#     query_engine = vector_index.as_query_engine(streaming=True, similarity_top_k=10)\n",
    "#     query_engine = vector_index.as_query_engine(similarity_top_k=10)\n",
    "    \n",
    "#     query_engine_tools = [\n",
    "#         QueryEngineTool(\n",
    "#             query_engine=query_engine,\n",
    "#             metadata=ToolMetadata(\n",
    "#                 name=\"GitHub Repo\",\n",
    "#                 description=\"The Github repo of a project\",\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "    \n",
    "#     query_engine = SubQuestionQueryEngine.from_defaults(\n",
    "#         query_engine_tools=query_engine_tools,\n",
    "#         service_context=service_context,\n",
    "#         use_async=True,\n",
    "#     )\n",
    "    #++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # Fusion\n",
    "    print(\"Fusion starting\")\n",
    "\n",
    "    \n",
    "\n",
    "    ## vector retriever\n",
    "    vector_retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "    ## bm25 retriever\n",
    "    bm25_retriever = BM25Retriever.from_defaults(\n",
    "        nodes=nodes,\n",
    "        similarity_top_k=2\n",
    "    )\n",
    "    \n",
    "    #for node in nodes:\n",
    "    #    display_source_node(node)\n",
    "    \n",
    "    global query_str\n",
    "    query_str = \"What is the repository about and what is the tech stack?\"\n",
    "    queries = generate_queries(llm, query_str, num_queries=4)\n",
    "    \n",
    "    global results_dict\n",
    "    results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])\n",
    "\n",
    "    final_results = fuse_results(results_dict)\n",
    "    \n",
    "    for n in final_results:\n",
    "        display_source_node(n, source_length=500)\n",
    "        \n",
    "    fusion_retriever = FusionRetriever(\n",
    "        llm, [vector_retriever, bm25_retriever], similarity_top_k=2\n",
    "    )\n",
    "\n",
    "    response_synthesizer = get_response_synthesizer(service_context=service_context)    \n",
    "    query_engine = RetrieverQueryEngine(\n",
    "        fusion_retriever,\n",
    "        response_synthesizer=response_synthesizer,\n",
    "    )\n",
    "    \n",
    "    response = query_engine.query(query_str)\n",
    "    print(str(response))\n",
    "    \n",
    "    #++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "#     Include a simple question to test\n",
    "#     intro_question = \"What is the repository about?\"\n",
    "#     print(f\"Test question: {intro_question}\")\n",
    "#     print('=' * 50)\n",
    "#     streaming_response = query_engine.query(intro_question)\n",
    "#     #streaming_response.print_response_stream()\n",
    "    \n",
    "#     print(f\"Answer: {textwrap.fill(str(streaming_response),100)} \\n\")\n",
    "    while True:\n",
    "        user_question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if user_question.lower() == 'exit':\n",
    "            print(\"Exiting, Thanks for chatting!\")\n",
    "            break\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Your question: {user_question}\")\n",
    "        \n",
    "        response = query_engine.query(user_question)\n",
    "        #streaming_response.print_response_stream()\n",
    "        print(f\"Answer: {textwrap.fill(str(response), 100)} \\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9a15dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672397a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754591bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de653f5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c4090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3091c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080b4672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90679a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1accb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.response_synthesizers import get_response_synthesizer\n",
    "\n",
    "response_synthesizer = get_response_synthesizer(service_context=service_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fef245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e6f10c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "099d261d",
   "metadata": {},
   "source": [
    "# Fusion Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566ea136",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "github_client = initialize_github_client()\n",
    "download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "github_url = \"https://github.com/zmusaddique/chatbot-restaurant\"\n",
    "owner, repo = parse_github_url(github_url)\n",
    "\n",
    "if validate_owner_repo(owner, repo):\n",
    "    loader = GithubRepositoryReader(\n",
    "        github_client,\n",
    "        owner=owner,\n",
    "        repo=repo,\n",
    "        filter_file_extensions=([\".py\", \".js\", \".ts\", \".md\", \".ipynb\"],\n",
    "                               GithubRepositoryReader.FilterType.INCLUDE,),\n",
    "        verbose=False,\n",
    "        concurrent_requests=10,\n",
    "    )\n",
    "    print(f\"Loading {repo} repository by {owner}\")\n",
    "    docs = loader.load_data(branch=\"main\")\n",
    "    print(\"Documeznts uploaded: \")\n",
    "    nodes = node_parser.get_nodes_from_documents(docs)\n",
    "\n",
    "vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=False,\n",
    "        runtime={\"tensor_db\":True},\n",
    "    )\n",
    "    \n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "vector_index = VectorStoreIndex(nodes, storage_context=storage_context, service_context=service_context, show_progress=True,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e6062",
   "metadata": {},
   "source": [
    "## Query Generation/Rewriting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3311b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"How do the models developed in this work compare to open-source chat models based on the benchmarks tested?\"\n",
    "query_gen_prompt_str = (\n",
    "    \"You are a helpful assistant that generates multiple search queries based on a \"\n",
    "    \"single input query. Generate {num_queries} search queries, one on each line, \"\n",
    "    \"related to the following input query:\\n\"\n",
    "    \"Query: {query}\\n\"\n",
    "    \"Queries:\\n\"\n",
    ")\n",
    "query_gen_prompt = PromptTemplate(query_gen_prompt_str)\n",
    "\n",
    "def generate_queries(llm, query_str: str, num_queries: int = 4):\n",
    "    fmt_prompt = query_gen_prompt.format(\n",
    "        num_queries=num_queries - 1, query=query_str\n",
    "    )\n",
    "    response = llm.complete(fmt_prompt)\n",
    "    queries = response.text.split(\"\\n\")\n",
    "    return queries\n",
    "\n",
    "queries = generate_queries(llm, query_str, num_queries=4)\n",
    "\n",
    "print(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e6dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8942a",
   "metadata": {},
   "source": [
    "## Perform Vector Search for Each Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb15e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.asyncio import tqdm\n",
    "\n",
    "async def run_queries(queries, retrievers):\n",
    "    \"\"\"Run queries against retrievers.\"\"\"\n",
    "    tasks = []\n",
    "    for query in queries:\n",
    "        for i, retriever in enumerate(retrievers):\n",
    "            tasks.append(retriever.aretrieve(query))\n",
    "\n",
    "    task_results = await tqdm.gather(*tasks)\n",
    "\n",
    "    results_dict = {}\n",
    "    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n",
    "        results_dict[(query, i)] = query_result\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c42bf85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get retrievers\n",
    "from llama_index.retrievers import BM25Retriever\n",
    "\n",
    "\n",
    "## vector retriever\n",
    "vector_retriever = vector_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "## bm25 retriever\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=vector_index.docstore, similarity_top_k=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f44798",
   "metadata": {},
   "source": [
    "#### Context\n",
    "reciprocal rank fusion: for each node, add up its reciprocal rank in every list where it’s retrieved.\n",
    "\n",
    "Then reorder nodes by highest score to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb41ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_results(results_dict, similarity_top_k: int = 2):\n",
    "    \"\"\"Fuse results.\"\"\"\n",
    "    k = 60.0  # `k` is a parameter used to control the impact of outlier rankings.\n",
    "    fused_scores = {}\n",
    "    text_to_node = {}\n",
    "\n",
    "    # compute reciprocal rank scores\n",
    "    for nodes_with_scores in results_dict.values():\n",
    "        for rank, node_with_score in enumerate(\n",
    "            sorted(\n",
    "                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n",
    "            )\n",
    "        ):\n",
    "            text = node_with_score.node.get_content()\n",
    "            text_to_node[text] = node_with_score\n",
    "            if text not in fused_scores:\n",
    "                fused_scores[text] = 0.0\n",
    "            fused_scores[text] += 1.0 / (rank + k)\n",
    "\n",
    "    # sort results\n",
    "    reranked_results = dict(\n",
    "        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    )\n",
    "\n",
    "    # adjust node scores\n",
    "    reranked_nodes: List[NodeWithScore] = []\n",
    "    for text, score in reranked_results.items():\n",
    "        reranked_nodes.append(text_to_node[text])\n",
    "        reranked_nodes[-1].score = score\n",
    "\n",
    "    return reranked_nodes[:similarity_top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99357067",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = fuse_results(results_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4b5e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ping www.google.com"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e54cd479",
   "metadata": {},
   "source": [
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import download_loader\n",
    "from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "from llama_index import VectorStoreIndex\n",
    "from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch and set API keys\n",
    "active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "\n",
    "\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)\n",
    "\n",
    "\n",
    "def initialize_github_client():\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check for GitHub Token\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not github_token:\n",
    "        raise EnvironmentError(\"GitHub token not found in environment variables\")\n",
    "\n",
    "    # Check for Activeloop Token\n",
    "    active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "    if not active_loop_token:\n",
    "        raise EnvironmentError(\"Activeloop token not found in environment variables\")\n",
    "\n",
    "    github_client = initialize_github_client()\n",
    "    download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "    github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "    owner, repo = parse_github_url(github_url)\n",
    "\n",
    "    while True:\n",
    "        owner, repo = parse_github_url(github_url)\n",
    "        if validate_owner_repo(owner, repo):\n",
    "            loader = GithubRepositoryReader(\n",
    "                github_client,\n",
    "                owner=owner,\n",
    "                repo=repo,\n",
    "                filter_file_extensions=(\n",
    "                    [\".py\", \".js\", \".ts\", \".md\"],\n",
    "                    GithubRepositoryReader.FilterType.INCLUDE,\n",
    "                ),\n",
    "                verbose=False,\n",
    "                concurrent_requests=5,\n",
    "            )\n",
    "            print(f\"Loading {repo} repository by {owner}\")\n",
    "            docs = loader.load_data(branch=\"main\")\n",
    "            print(\"Documents uploaded:\")\n",
    "            for doc in docs:\n",
    "                print(doc.metadata)\n",
    "            break  # Exit the loop once the valid URL is processed\n",
    "        else:\n",
    "            print(\"Invalid GitHub URL. Please try again.\")\n",
    "            github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "\n",
    "    print(\"Uploading to vector store...\")\n",
    "\n",
    "    # ====== Create vector store and upload data ======\n",
    "\n",
    "    vector_store = DeepLakeVectorStore(\n",
    "        dataset_path=dataset_path,\n",
    "        overwrite=True,\n",
    "        runtime={\"tensor_db\": True},\n",
    "    )\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context)\n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # Include a simple question to test.\n",
    "    intro_question = \"What is the repository about?\"\n",
    "    print(f\"Test question: {intro_question}\")\n",
    "    print(\"=\" * 50)\n",
    "    answer = query_engine.query(intro_question)\n",
    "\n",
    "    print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "    while True:\n",
    "        user_question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if user_question.lower() == \"exit\":\n",
    "            print(\"Exiting, thanks for chatting!\")\n",
    "            break\n",
    "\n",
    "        print(f\"Your question: {user_question}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        answer = query_engine.query(user_question)\n",
    "        print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc5d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
