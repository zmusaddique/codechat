{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "985e67fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dfd54bb1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the GitHub repository URL: https://github.com/zmusaddique/chatbot-restaurant\n",
      "Loading chatbot-restaurant repository by zmusaddique\n",
      "Documents uploaded:\n",
      "{'file_path': 'FoodChatBot/db_helper.py', 'file_name': 'db_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/db_helper.py'}\n",
      "{'file_path': 'FoodChatBot/frontend/frontend_server.py', 'file_name': 'frontend_server.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/frontend/frontend_server.py'}\n",
      "{'file_path': 'FoodChatBot/generic_helper.py', 'file_name': 'generic_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/generic_helper.py'}\n",
      "{'file_path': 'FoodChatBot/main.py', 'file_name': 'main.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/FoodChatBot/main.py'}\n",
      "{'file_path': 'README.md', 'file_name': 'README.md', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/README.md'}\n",
      "{'file_path': 'backend/db_helper.py', 'file_name': 'db_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/db_helper.py'}\n",
      "{'file_path': 'backend/extra/extra.py', 'file_name': 'extra.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/extra/extra.py'}\n",
      "{'file_path': 'backend/generic_helper.py', 'file_name': 'generic_helper.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/generic_helper.py'}\n",
      "{'file_path': 'backend/main.py', 'file_name': 'main.py', 'url': 'https://github.com/zmusaddique/chatbot-restaurant/blob/main/backend/main.py'}\n",
      "Uploading to vector store...\n",
      "Test question: What is the repository about?\n",
      "==================================================\n",
      "Answer:  The repository contains the code for a chatbot implemented using Dialogflow for natural language\n",
      "understanding and FastAPI for the backend server. The chatbot allows users to interact with it for\n",
      "placing food orders, tracking orders, adding and removing items from their order, and completing\n",
      "orders.  If you are interested in a career switch to data analysis, it is definitely possible,\n",
      "regardless of your age or background. Many individuals have successfully made the transition,\n",
      "including those with engineering backgrounds and business degrees. For example, watch these videos\n",
      "for success stories of Mechanical Engineers and B.Com graduates transitioning into data analysis:\n",
      "(1) <https://www.youtube.com/watch?v=4BLxapDqrlA> (2) <https://www.youtube.com/watch?v=yKB6EUbGamo>\n",
      "(3) <https://www.youtube.com/watch?v=in3IB45YEgY> (4) <https://www.youtube.com/watch?v=lqEzYDuTnvU>\n",
      "The data analysis field requires a strong foundation in Excel, Power BI, SQL, and other relevant\n",
      "tools. With dedication \n",
      "\n",
      "Please enter your question (or type 'exit' to quit): What is the server written in?\n",
      "Your question: What is the server written in?\n",
      "==================================================\n",
      "Answer:  The server is written in Python based on the provided code snippets. The first code snippet is for\n",
      "serving a static HTML file using Python's built-in HTTP server, and the second code snippet is a\n",
      "Python module for handling some generic helper functions. \n",
      "\n",
      "Please enter your question (or type 'exit' to quit): exit\n",
      "Exiting, thanks for chatting!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import textwrap\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import download_loader, ServiceContext\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_hub.github_repo import GithubRepositoryReader, GithubClient\n",
    "from llama_index import VectorStoreIndex\n",
    "# from llama_index.vector_stores import DeepLakeVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "from llama_index.llms import HuggingFaceInferenceAPI\n",
    "import chromadb\n",
    "import re\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "llm = HuggingFaceInferenceAPI(\n",
    "    model_name=\"mistralai/Mistral-7B-Instruct-v0.2\",  # replace with your model name\n",
    "    context_window=2048,  # to use refine\n",
    "    token=os.getenv('HUGGINGFACEHUB_API_TOKEN'),  # replace with your HuggingFace token\n",
    ")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# Fetch and set API keys\n",
    "# active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "# dataset_path = os.getenv(\"DATASET_PATH\")\n",
    "\n",
    "\n",
    "def parse_github_url(url):\n",
    "    pattern = r\"https://github\\.com/([^/]+)/([^/]+)\"\n",
    "    match = re.match(pattern, url)\n",
    "    return match.groups() if match else (None, None)\n",
    "\n",
    "\n",
    "def validate_owner_repo(owner, repo):\n",
    "    return bool(owner) and bool(repo)\n",
    "\n",
    "\n",
    "def initialize_github_client():\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    return GithubClient(github_token)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check for GitHub Token\n",
    "    github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not github_token:\n",
    "        raise EnvironmentError(\"GitHub token not found in environment variables\")\n",
    "\n",
    "    # Check for Activeloop Token\n",
    "    active_loop_token = os.getenv(\"ACTIVELOOP_TOKEN\")\n",
    "    if not active_loop_token:\n",
    "        raise EnvironmentError(\"Activeloop token not found in environment variables\")\n",
    "\n",
    "    github_client = initialize_github_client()\n",
    "    download_loader(\"GithubRepositoryReader\")\n",
    "\n",
    "    github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "    owner, repo = parse_github_url(github_url)\n",
    "\n",
    "    while True:\n",
    "        owner, repo = parse_github_url(github_url)\n",
    "        if validate_owner_repo(owner, repo):\n",
    "            loader = GithubRepositoryReader(\n",
    "                github_client,\n",
    "                owner=owner,\n",
    "                repo=repo,\n",
    "                filter_file_extensions=(\n",
    "                    [\".py\", \".js\", \".ts\", \".md\"],\n",
    "                    GithubRepositoryReader.FilterType.INCLUDE,\n",
    "                ),\n",
    "                verbose=False,\n",
    "                concurrent_requests=5,\n",
    "            )\n",
    "            print(f\"Loading {repo} repository by {owner}\")\n",
    "            docs = loader.load_data(branch=\"main\")\n",
    "            print(\"Documents uploaded:\")\n",
    "            for doc in docs:\n",
    "                print(doc.metadata)\n",
    "            break  # Exit the loop once the valid URL is processed\n",
    "        else:\n",
    "            print(\"Invalid GitHub URL. Please try again.\")\n",
    "            github_url = input(\"Please enter the GitHub repository URL: \")\n",
    "\n",
    "    print(\"Uploading to vector store...\")\n",
    "\n",
    "    # ====== Create vector store and upload data ======\n",
    "\n",
    "#     vector_store = DeepLakeVectorStore(\n",
    "#         dataset_path=dataset_path,\n",
    "#         overwrite=True,\n",
    "#         runtime={\"tensor_db\": True},\n",
    "#     )\n",
    "    #create client and a new collection\n",
    "    chroma_client = chromadb.EphemeralClient()\n",
    "    chroma_collection = chroma_client.get_or_create_collection(\"codechat\")\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    service_context = ServiceContext.from_defaults(llm = llm, embed_model=embed_model)\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        docs, \n",
    "        storage_context=storage_context, \n",
    "        service_context=service_context\n",
    "    )\n",
    "    \n",
    "    query_engine = index.as_query_engine()\n",
    "\n",
    "    # Include a simple question to test.\n",
    "    intro_question = \"What is the repository about?\"\n",
    "    print(f\"Test question: {intro_question}\")\n",
    "    print(\"=\" * 50)\n",
    "    answer = query_engine.query(intro_question)\n",
    "\n",
    "    print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "    while True:\n",
    "        user_question = input(\"Please enter your question (or type 'exit' to quit): \")\n",
    "        if user_question.lower() == \"exit\":\n",
    "            print(\"Exiting, thanks for chatting!\")\n",
    "            break\n",
    "\n",
    "        print(f\"Your question: {user_question}\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        answer = query_engine.query(user_question)\n",
    "        print(f\"Answer: {textwrap.fill(str(answer), 100)} \\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7bbcee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
